---
id: module4
title: Module 4 – Vision-Language-Action (VLA)
sidebar_position: 4
---

# Module 4 – Vision-Language-Action (VLA)

## Focus  
**The convergence of LLMs and Robotics**

This module connects large language models with real robotic actions, enabling humanoid robots to understand voice commands and perform intelligent task planning.

## Chapters in This Module

- Voice-to-Action: Using OpenAI Whisper for Voice Commands  
- Cognitive Planning: Using LLMs to Translate Natural Language into ROS 2 Actions  
- Capstone Project: The Autonomous Humanoid  

### Capstone Project
A simulated humanoid robot will:
- Receive a voice command  
- Plan a navigation path  
- Avoid obstacles  
- Detect objects using computer vision  
- Manipulate the target object autonomously  

This is the final integration of all modules.
